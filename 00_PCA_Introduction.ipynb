{"cells":[{"cell_type":"markdown","metadata":{"id":"uaHqELK1Uqa0"},"source":["## What you'll learn in this course\n","\n","* What is Dimensionality Reduction and why do we use it\n","* What is PCA\n","* The Intuition Behind the computation\n","\n","## **Dimensionality Reduction**\n","\n","There is this misconception in data science that the more explanatory variables we have at our disposal, the better the model we can derive from them.\n","\n","The key to effective data science is producing useful results within a given context. Sometimes, having too many variables can introduce redundancy and make interpretation difficult.\n","\n","To address this, we use dimensionality reduction techniques. These methods allow us to summarize data efficiently, reducing the number of features while preserving the most important information.\n","\n","---\n","\n","## **Why Do We Use Dimensionality Reduction**\n","\n","### 1. Visualisation\n","\n","Imagine you have a dataset with four dimensions (4-D). Since humans can only visualize up to three dimensions, plotting such data directly is impossible. However, by reducing the dimensionality (e.g., from 4D to 2D), we can project the data into a lower-dimensional space and visualize it more easily.\n","\n","\n","### 2. Noise Reduction\n","\n","It is common in data science to have redundant or highly correlated variables that describe the same phenomenon. Reducing the number of dimensions can filter out noise and highlight the most relevant patterns. Think about a dataset of blurry images with excessive noise.\n","\n","---\n","\n","## **Principal Component Analysis (PCA)**\n","\n","PCA is the most famous algorithm for dimensionality reduction.\n","\n","The idea of this unsupervised algorithm is to create a linear combination of features that will transform your initial dataset into a smaller dataset.\n","\n","\n","\n","<img src=\"https://raw.githubusercontent.com/sbendimerad/sklearn_course_demo/main/images/image1.png\" alt=\"PCA_logic\"/>\n","\n","To achieve this, the original dataset is multiplied by a matrix of vectors, named Eigen Vectors. Eigen Vectors have key properties:\n","\n","* They define new axes (principal components) that best capture the variance in the data.\n","* They summarize the original data, preserving the most important patterns with minimal information loss.\n","\n","\n","<img src=\"https://raw.githubusercontent.com/sbendimerad/sklearn_course_demo/main/images/image3.png\" alt=\"UA\"/>\n","\n"]},{"cell_type":"markdown","source":["---\n","\n","# **How It Works?**\n","\n","How do we find these **eigenvectors**? We follow **four key steps**:\n","\n","## **1. Normalization**  \n","\n","This is a standard preprocessing step in Machine Learning. We apply the following formula to all data points in the dataset:\n","\n","$$\n","z_i = \\frac{x_i - \\mu}{\\sigma}\n","$$\n","\n","Where:  \n","- $\\mu$ is the **mean** of the feature  \n","- $\\sigma$ is the **standard deviation**  \n","\n","This ensures that all features have the same scale, preventing bias in the analysis.\n","\n","---\n","\n","## **2. Compute the Covariance Matrix**  \n","\n","Covariance represents how two variables in our dataset are related to each other. Below is an example of a **covariance matrix**:\n","\n","\n","<img src=\"https://raw.githubusercontent.com/sbendimerad/sklearn_course_demo/main/images/image4.png\" alt=\"PCA_logic\"/>\n","\n","- **Diagonal values** → variance of each variable  \n","- **Off-diagonal values** → covariance between variables  \n","\n","### **Why is this important?**  \n","The goal of PCA is to **summarize information** efficiently. Ideally, we want to transform our data into a new coordinate system where the **covariance matrix contains only non-zero values on the diagonal**.  \n","\n","This means:  \n","- We have **new uncorrelated features** (principal components).  \n","- Redundant information is **removed**.  \n","- Noise is reduced, and we keep only the **most important information**.  \n"],"metadata":{"id":"3UkLYzlDVmMK"}},{"cell_type":"markdown","metadata":{"id":"NgJi6wVbUqa3"},"source":["## **3. Compute SVD**  \n","\n","PCA relies on **Singular Value Decomposition (SVD)** to break down the original data matrix $A$ into three matrices:  \n","\n","$$\n","A = U \\Sigma V^\\intercal\n","$$\n","\n","Where:  \n","- $U$ contains the **eigenvectors** of $AA^\\intercal$ (new axes for projection).  \n","- $\\Sigma$ is a diagonal matrix of **eigenvalues** (representing the importance of each axis).  \n","- $V^\\intercal$ contains the **eigenvectors** of $A^\\intercal A$.  \n","\n","Finding $U$ means we've identified the best directions to **project** our data into a lower-dimensional space.\n","\n","---\n","\n","## **4. Apply PCA**  \n","\n","To get our **principal components**, we simply multiply:  \n","\n","$$\n","A' = A U\n","$$\n","\n","This **transforms** the data into a new space where features are uncorrelated and ordered by importance.  \n","\n","Thanks to this transformation, we obtain **new features** (principal components) that **summarize the data** while reducing its dimensionality.  \n","\n","Imagine our original dataset contained variables like **age, salary, number of children, and years of experience**. After PCA, the data is summarized into **two new features (PC1 and PC2)**—each representing a linear combination of the original variables.  \n","\n","The number of principal components is always **less than or equal to** the number of original features.\n","\n","\n","\n","\n","<img src=\"https://raw.githubusercontent.com/sbendimerad/sklearn_course_demo/main/images/image1.png\" alt=\"UA\"/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1cxcJpqGAP7qKy-RIqik8cln8d5Uckz3U","timestamp":1740943607528}]}},"nbformat":4,"nbformat_minor":0}